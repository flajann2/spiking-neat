* Notes
  These notes are for me only, and I do not promise to
  keep them up-to-date or even accurate.
  
  The official docs are in the README.org.

** Basic design
   + We shall have, instead of negative weights,
     negative or inhibitory neurons, which will send
     the weights as negative to the neuros its conneted
     to.
   + Instead of averaging, we will use spikes. There
     are the time instants, and for each time instant
     the inputs (and their weights) for each neuron
     will be tallied, and if it reaches firing
     threshold, said neuron will fire its own pulse.
   + Some neurons will continually fire pulses at
     regular intervals. This will serve as a bias.

     I actually think there should be a number of bias neurons
     firing at regular intervals. The firing rates should not
     necessairily be random, either. Maybe Fibonancii? Maybe
     these rates themselves should be evolved?
   + In the Stanley algorithm, the neurons and genes are indexed
     by integers. To promote efficient execution of these neurons,
     should we maintain that? Perhaps we should simply create the 
     IR from the indicies directly.

** Miscellaneous
   + I had to remove Node and Connection from Gene
     and make them separate. Haskell doesnät like to
     array the type constructor otherwise.
** Hebbian Learning
   I need to explore the inclusion of Hebbian leaarning for S-NEAT.
   + w ij ←w ij +η⋅r i ⋅r j -- just look up the actual formula.
   + How will this work in the spiking context? Do I allow the
     weights to change as the critter is working, as opposed to only
     doing that during evolution?
   + Do I Lamackian those changes back to the genes?
** Spiking
   I have not understood how I shall exactly implement
   spiking. Do I evaluate the entire critter per spike?
   Or do I evaluate each neuron per spike?
   + Whole critter eval
     This would entail pushing out all the evaluations
     to the output neurons per spike, and during that
     spike allowing for settlement due to loops in neurons.
   + Per Spike Evaluation
     This would be slower, but a bit closer to the
     actual biological context. But a lot more compute
     intensive, meaning more parallel computation perhaps.
   + I am leaning toward the latter. Neuron per spike. We will
     have to optimize for speed later.
*** Neuron per spike
    A vector of neurons -- or nodes -- will have to be
    maintained somewhere, and I was thinking floats. But what
    if we used Complex numbers or even quaternions instead?

    I will need to make it all generic, which Haskell
    will allow me to do easily enough.
** Generic Floats
   I am calling them that for now,
   but really I want to be able to
   use complex numbers or even quaturnions
   instead. I do not know where this
   will lead. It may make it able to
   latch on to deeper nuances of learning.
   

​

